\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[provide=*,french]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{xcolor}

\geometry{margin=2.5cm}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}

\title{
  \textbf{Planification de Trajectoire pour Robot Autonome \\
  en Environnement Dynamique par M\'ethodes de \\
  Monte Carlo Tree Search (MCTS)} \\
  \vspace{0.5cm}
  \large Projet -- Cours de M\'ethodes de Monte Carlo \\
  M2 IASD
}

\author{
  KEITA Mamadi \and Afi Skander
}

\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\begin{abstract}
Ce rapport pr\'esente l'\'etude et l'impl\'ementation d'une m\'ethode de planification de trajectoire pour un robot mobile autonome \'evoluant dans un environnement comportant des obstacles statiques et dynamiques. L'approche repose sur l'utilisation des algorithmes de Monte Carlo Tree Search (MCTS), en particulier UCT, RAVE et GRAVE, pour permettre au robot de d\'eterminer un chemin optimal entre un point de d\'epart et une destination cible. Nous traitons les probl\'ematiques d'\'evitement d'obstacles, d'adaptation face aux obstacles mobiles, de re-planification en temps r\'eel et de gestion des incertitudes capteurs. Les r\'esultats exp\'erimentaux montrent que GRAVE offre le meilleur compromis performance/temps de calcul, notamment en environnement dynamique.
\end{abstract}
\thispagestyle{empty}
\newpage

\tableofcontents
\thispagestyle{empty}
\newpage

% ============================================================================
\section{Introduction}
% ============================================================================

\subsection{Contexte et motivation}

La planification de trajectoire pour robots autonomes est un probl\`eme fondamental en robotique mobile. Un robot doit naviguer depuis un point de d\'epart vers une destination en \'evitant les obstacles, tout en s'adaptant aux changements de l'environnement en temps r\'eel. Ce probl\`eme se pose dans de nombreuses applications~: robots de service, v\'ehicules autonomes, drones, robots d'entrep\^ot, etc.

Les m\'ethodes classiques de planification (A*, RRT, RRT*) supposent g\'en\'eralement un environnement statique et parfaitement connu. Or, dans la r\'ealit\'e, l'environnement est \textbf{dynamique} (personnes, autres robots qui se d\'eplacent) et \textbf{partiellement observable} (incertitude des capteurs).

\subsection{Pourquoi Monte Carlo Tree Search ?}

Le MCTS, introduit ind\'ependamment par \cite{kocsis2006bandit} et \cite{coulom2006efficient}, est particuli\`erement adapt\'e \`a ce probl\`eme pour plusieurs raisons~:

\begin{enumerate}
  \item \textbf{Gestion de l'incertitude}~: Les playouts al\'eatoires permettent d'\'echantillonner les futurs possibles de l'environnement, incluant les mouvements impr\'evisibles des obstacles dynamiques.
  \item \textbf{Planification anytime}~: MCTS peut \^etre interrompu \`a tout moment et fournir la meilleure action trouv\'ee jusqu'alors, ce qui est essentiel pour le temps r\'eel.
  \item \textbf{Pas de mod\`ele complet n\'ecessaire}~: Contrairement \`a la programmation dynamique, MCTS n'a besoin que d'un simulateur de l'environnement.
  \item \textbf{\'Equilibre exploration/exploitation}~: La formule UCT (\S\ref{sec:uct}) garantit un \'equilibre optimal entre l'exploration de nouvelles trajectoires et l'exploitation des trajectoires prometteuses.
\end{enumerate}

L'application du MCTS \`a la planification robot a \'et\'e valid\'ee r\'ecemment par plusieurs travaux~: \cite{dam2022monte} prouvent la convergence exponentielle de UCT pour le path planning, \cite{bonanni2025mcts} combinent MCTS avec les Velocity Obstacles pour les environnements denses, et \cite{riviere2024sets} publient dans \textit{Science Robotics} une m\'ethode MCTS pour les syst\`emes dynamiques continus.

\subsection{Objectifs du projet}

Ce projet vise \`a~:
\begin{itemize}
  \item Impl\'ementer un environnement de simulation avec obstacles statiques et dynamiques
  \item Impl\'ementer et comparer quatre algorithmes MCTS~: Flat MC, UCT, RAVE, GRAVE
  \item \'Etudier la re-planification en temps r\'eel
  \item \'Evaluer la robustesse face au bruit capteur
  \item Analyser l'impact des param\`etres (budget de simulations, constante d'exploration)
\end{itemize}

% ============================================================================
\section{\'Etat de l'art}
% ============================================================================

\subsection{Fondements du MCTS}

\subsubsection{Principe de Monte Carlo}

Le principe fondamental des m\'ethodes de Monte Carlo est d'estimer une quantit\'e par \'echantillonnage al\'eatoire plut\^ot que par calcul exhaustif. Appliqu\'e \`a la recherche dans les arbres de d\'ecision, cela donne le MCTS~: au lieu d'explorer tout l'arbre (impossible pour les grands espaces d'\'etats), on \'echantillonne des trajectoires al\'eatoires (\textit{playouts}) pour estimer la valeur de chaque action~\citep{browne2012survey}.

\subsubsection{UCT~: Upper Confidence Bound for Trees}
\label{sec:uct}

UCT~\citep{kocsis2006bandit} applique le principe des bandits multi-bras \`a la recherche dans les arbres. La formule de s\'election est~:

\begin{equation}
  \text{UCT}(i) = \frac{w_i}{n_i} + C \sqrt{\frac{\ln N}{n_i}}
  \label{eq:uct}
\end{equation}

o\`u $w_i$ est la somme des r\'ecompenses du n\oe{}ud $i$, $n_i$ le nombre de visites, $N$ le nombre de visites du parent, et $C$ la constante d'exploration (typiquement $C = \sqrt{2}$).

Le premier terme $w_i/n_i$ favorise l'\textbf{exploitation} (noeuds avec de bons r\'esultats), tandis que le second terme $C\sqrt{\ln N / n_i}$ favorise l'\textbf{exploration} (noeuds peu visit\'es).

\subsubsection{Les quatre phases de MCTS}

L'algorithme MCTS r\'ep\`ete it\'erativement quatre phases~:
\begin{enumerate}
  \item \textbf{S\'election}~: Descendre dans l'arbre en choisissant les noeuds avec le meilleur score UCT.
  \item \textbf{Expansion}~: Ajouter un nouveau noeud enfant pour une action non explor\'ee.
  \item \textbf{Simulation}~: Jouer un playout al\'eatoire depuis le nouveau noeud.
  \item \textbf{R\'etropropagation}~: Remonter le r\'esultat et mettre \`a jour les statistiques.
\end{enumerate}

\subsection{Am\'eliorations~: RAVE et GRAVE}

\subsubsection{RAVE~: Rapid Action Value Estimation}

RAVE~\citep{gelly2007combining} acc\'el\`ere l'apprentissage en utilisant les statistiques AMAF (All Moves As First)~: si une action $m$ appara\^it \textit{n'importe o\`u} dans un playout gagnant, elle est probablement bonne. La formule combine UCT et AMAF~:

\begin{equation}
  \text{Valeur}(m) = (1 - \beta_m) \cdot \text{UCT}_m + \beta_m \cdot \text{AMAF}_m
  \label{eq:rave}
\end{equation}

avec $\beta_m = \frac{p_m^{\text{AMAF}}}{p_m^{\text{AMAF}} + p_m + b \cdot p_m^{\text{AMAF}} \cdot p_m}$, o\`u $b$ est un biais (typiquement $b = 10^{-4}$).

\subsubsection{GRAVE~: Generalized RAVE}

GRAVE~\citep{cazenave2015generalized} r\'esout le probl\`eme de fiabilit\'e des statistiques AMAF pour les noeuds peu visit\'es~: au lieu d'utiliser les statistiques AMAF du noeud courant, on utilise celles du premier anc\^etre ayant plus de $n_{\text{ref}}$ visites (typiquement $n_{\text{ref}} = 50$).

\subsection{MCTS pour la planification robot}

L'application de MCTS \`a la planification de trajectoire est un domaine de recherche actif~:

\begin{itemize}
  \item \cite{dam2022monte} introduisent Monte-Carlo Path Planning (MCPP) et prouvent la convergence exponentielle vers le chemin optimal, avec des exp\'eriences sur un robot r\'eel.
  \item \cite{li2023self} proposent SL-MCTS combinant MCTS avec un r\'eseau de neurones inspir\'e d'AlphaZero pour l'auto-apprentissage de la politique de recherche.
  \item \cite{riviere2024sets} publient dans \textit{Science Robotics} une m\'ethode utilisant l'expansion spectrale pour g\'erer les espaces continus.
  \item \cite{bonanni2025mcts} combinent MCTS avec les Velocity Obstacles pour la navigation s\^ure dans des environnements denses avec jusqu'\`a 40 obstacles dynamiques.
  \item \cite{eiffert2020path} utilisent des RNN g\'en\'eratives dans MCTS pour pr\'edire le comportement des pi\'etons.
  \item \cite{janson2015monte} proposent MCMP pour l'optimisation de trajectoire sous incertitude avec des techniques de r\'eduction de variance.
\end{itemize}

Le survey r\'ecent de \cite{swiechowski2023mcts} recense les applications modernes de MCTS au-del\`a des jeux, incluant la planification et la robotique.

\subsection{Progressive Widening}

Pour les probl\`emes avec des espaces d'actions continus (comme la robotique), \cite{chaslot2008progressive} et \cite{couetoux2011continuous} proposent le \textit{Progressive Widening}~: on n'ajoute un nouveau fils que lorsque $n(s)^{p_w} \geq |\text{enfants}(s)|$. Cela garantit qu'on explore en profondeur avant d'\'elargir l'arbre.

% ============================================================================
\section{M\'ethodologie}
% ============================================================================

\subsection{Mod\'elisation du probl\`eme}

\subsubsection{Environnement}

L'environnement est une grille 2D de taille $N \times N$ o\`u chaque cellule peut \^etre~:
\begin{itemize}
  \item Libre (le robot peut y passer)
  \item Obstacle statique (mur, meuble -- permanent)
  \item Obstacle dynamique (personne, autre robot -- se d\'eplace)
  \item Position du robot
  \item Position objectif
\end{itemize}

\subsubsection{Mod\'elisation MDP}

Nous formalisons le probl\`eme comme un Processus de D\'ecision Markovien (MDP)~:
\begin{itemize}
  \item \textbf{\'Etats}~: $s = (\text{pos\_robot}, \text{pos\_obstacles}, t)$
  \item \textbf{Actions}~: 9 actions (8 directions + attendre)
  \item \textbf{Transitions}~: d\'eterministes pour le robot, stochastiques pour les obstacles
  \item \textbf{R\'ecompenses}~:
    \begin{itemize}
      \item Objectif atteint~: $+100 \times (1 - t/t_{\max})$ (bonus de rapidit\'e)
      \item Collision~: $-100$
      \item Chaque pas~: $-1$ (encourage les chemins courts)
    \end{itemize}
\end{itemize}

La fonction de r\'ecompense avec bonus de rapidit\'e est inspir\'ee de la \textit{discounting heuristic} du cours (section~20.1), qui p\'enalise les solutions longues.

\subsubsection{Obstacles dynamiques}

Trois patterns de mouvement sont impl\'ement\'es~:
\begin{enumerate}
  \item \textbf{Lin\'eaire}~: va-et-vient en ligne droite (pr\'edictible)
  \item \textbf{Al\'eatoire}~: direction choisie al\'eatoirement \`a chaque pas (impr\'edictible)
  \item \textbf{Circulaire}~: mouvement circulaire autour d'un point
\end{enumerate}

\subsubsection{Incertitude capteur}

Le bruit capteur est mod\'elis\'e par un param\`etre $\sigma \in [0, 1]$~: avec probabilit\'e $\sigma$, une cellule observ\'ee peut \^etre invers\'ee (libre $\leftrightarrow$ occup\'ee). Cela cr\'ee un probl\`eme d'\textbf{information imparfaite}, analogue aux jeux \'etudi\'es dans le cours (section~19).

\subsection{Algorithmes impl\'ement\'es}

\subsubsection{Flat Monte Carlo (baseline)}

Pour chaque action l\'egale, on effectue $N/K$ playouts al\'eatoires ($K$ = nombre d'actions) et on choisit l'action avec la meilleure r\'ecompense moyenne. C'est la m\'ethode la plus simple (cours section~1.2).

\subsubsection{UCT}

Impl\'ementation standard de UCT avec les quatre phases de MCTS. Nous proposons deux variantes de playout~:
\begin{itemize}
  \item \textbf{Playout al\'eatoire}~: actions choisies uniform\'ement
  \item \textbf{Playout heuristique}~: probabilit\'e proportionnelle \`a $\exp(-d)$ o\`u $d$ est la distance Manhattan \`a l'objectif apr\`es l'action (\'echantillonnage de Gibbs, cours section~10.2)
\end{itemize}

\subsubsection{RAVE}

UCT avec statistiques AMAF. Apr\`es chaque playout, on enregistre toutes les actions jou\'ees et on met \`a jour les statistiques AMAF de chaque noeud anc\^etre. La formule~\eqref{eq:rave} combine les deux sources d'information.

\subsubsection{GRAVE}

Am\'elioration de RAVE utilisant les statistiques AMAF du premier anc\^etre fiable (ayant au moins $n_{\text{ref}} = 50$ visites).

\subsection{Re-planification dynamique}

La strat\'egie de re-planification est de type \textit{receding horizon}~: \`a chaque pas de temps, le robot~:
\begin{enumerate}
  \item Observe l'environnement (avec bruit capteur \'eventuel)
  \item Lance une recherche MCTS pour choisir la prochaine action
  \item Ex\'ecute l'action
  \item R\'ep\`ete
\end{enumerate}

Nous proposons \'egalement un planificateur \textbf{adaptatif} qui ajuste le budget de simulations selon le niveau de danger (proximit\'e des obstacles dynamiques, \'etroitesse du passage).

\subsection{Planification multi-mondes}

Inspir\'e de PIMC (Perfect Information Monte Carlo, cours section~19.2), nous impl\'ementons un planificateur qui \'echantillonne $N$ mondes possibles (perturbations des positions des obstacles dynamiques) et choisit l'action par vote majoritaire. Cela est analogue \`a la Root Parallelization (cours section~7.1).

% ============================================================================
\section{Impl\'ementation}
% ============================================================================

\subsection{Architecture logicielle}

Le code est organis\'e en modules Python~:
\begin{itemize}
  \item \texttt{environment.py}~: Environnement de grille 2D, obstacles, capteurs
  \item \texttt{mcts\_base.py}~: Flat MC et UCT
  \item \texttt{mcts\_rave.py}~: RAVE et GRAVE
  \item \texttt{mcts\_dynamic.py}~: Re-planification dynamique, planification adaptative
  \item \texttt{visualization.py}~: Visualisation matplotlib
  \item Tests unitaires et scripts d'exp\'erimentation
\end{itemize}

\subsection{D\'etails techniques}

\subsubsection{Conformit\'e avec le cours}

L'impl\'ementation respecte les recommandations du cours (section~25)~:
\begin{itemize}
  \item Chaque \'etat est \textbf{copi\'e} avant simulation (erreur~\#1)
  \item Les playouts ont une \textbf{limite de pas} pour \'eviter les boucles infinies (erreur~\#6)
  \item Les r\'esultats sont moyenn\'es sur \textbf{suffisamment de parties} (erreur~\#5)
  \item L'action finale est choisie par \textbf{nombre de visites} (plus robuste que la valeur Q)
\end{itemize}

\subsubsection{Param\`etres}

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Param\`etre} & \textbf{Symbole} & \textbf{Valeur} \\
\hline
Constante d'exploration UCT & $C$ & $\sqrt{2} \approx 1.414$ \\
Budget de simulations & $N$ & 500 (d\'efaut) \\
Biais RAVE & $b$ & $10^{-4}$ \\
Seuil GRAVE & $n_{\text{ref}}$ & 50 \\
Profondeur max arbre & -- & 15 niveaux \\
Profondeur max playout & -- & 50 pas \\
Pas max par \'episode & $t_{\max}$ & 200 \\
Score position & $\gamma^d$ & $\gamma = 0.9$, $d$ = dist. Manhattan \\
\hline
\end{tabular}
\caption{Param\`etres par d\'efaut (conform\'ement aux recommandations du cours, section~25.2).}
\label{tab:params}
\end{table}

% ============================================================================
\section{Exp\'erimentations}
% ============================================================================

\subsection{Protocole exp\'erimental}

Six exp\'erimentations sont conduites, chacune r\'ep\'et\'ee 8 fois avec des graines al\'eatoires diff\'erentes pour obtenir des r\'esultats statistiquement significatifs (cf. cours section~25.3, erreur~\#5).

\begin{enumerate}
  \item Comparaison des algorithmes sur sc\'enario statique
  \item Impact du budget de simulations
  \item Impact de la constante d'exploration $C$
  \item Performance en environnement dynamique
  \item Robustesse au bruit capteur
  \item Sc\'enario de passage \'etroit
\end{enumerate}

\subsection{Sc\'enarios de test}

\begin{itemize}
  \item \textbf{Sc\'enario simple}~: Grille $15 \times 15$, obstacles statiques (3 murs internes), d\'epart $(1,1)$, objectif $(13,13)$
  \item \textbf{Sc\'enario dynamique}~: Grille $15 \times 15$, 3 obstacles dynamiques (lin\'eaire, vertical, al\'eatoire)
  \item \textbf{Passage \'etroit}~: Mur traversant avec un seul passage, obstacle dynamique bloquant
  \item \textbf{Bruit capteur}~: Niveaux $\sigma \in \{0\%, 5\%, 10\%, 20\%, 30\%\}$
\end{itemize}

\subsection{R\'esultats et analyse}

\subsubsection{Exp. 1~: Comparaison des algorithmes}

Sur le sc\'enario statique (grille $15 \times 15$, $N = 500$ simulations, 8 r\'ep\'etitions)~:

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Algorithme} & \textbf{Taux de succ\`es} & \textbf{Pas moyens ($\pm\sigma$)} & \textbf{Temps/recherche (s)} \\
\hline
Flat MC & 100\% & 22.8 $\pm$ 2.9 & 0.261 \\
UCT & 100\% & 23.0 $\pm$ 4.3 & 0.319 \\
RAVE & 100\% & 22.5 $\pm$ 3.7 & 0.339 \\
GRAVE & 100\% & \textbf{22.4 $\pm$ 3.6} & 0.337 \\
\hline
\end{tabular}
\caption{Comparaison des algorithmes sur sc\'enario statique ($N=500$, 8 r\'ep\'etitions). GRAVE obtient la trajectoire la plus courte en moyenne avec l'\'ecart-type le plus faible.}
\label{tab:exp1}
\end{table}

\textbf{Analyse}~: Sur ce sc\'enario simple, tous les algorithmes atteignent 100\% de succ\`es. La hi\'erarchie observ\'ee (GRAVE $\leq$ RAVE $\leq$ Flat MC $<$ UCT en nombre de pas) est coh\'erente avec les r\'esultats du cours~: les statistiques AMAF (RAVE/GRAVE) permettent un apprentissage plus rapide et des trajectoires plus courtes. Les diff\'erences sont faibles car la grille $15 \times 15$ est relativement simple (distance Manhattan de 24 entre d\'epart et objectif).

\subsubsection{Exp. 2~: Budget de simulations}

L'impact du budget est mesur\'e avec l'algorithme GRAVE sur le sc\'enario statique~:

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Budget $N$} & \textbf{Taux de succ\`es} & \textbf{Pas moyens} & \textbf{Temps/recherche (s)} \\
\hline
100 & 100\% & 43.4 & 0.067 \\
300 & 100\% & 28.9 & 0.202 \\
500 & 100\% & 22.4 & 0.338 \\
1000 & 100\% & \textbf{20.0} & 0.678 \\
\hline
\end{tabular}
\caption{Impact du budget de simulations sur GRAVE. Rendements d\'ecroissants au-del\`a de $N=500$.}
\label{tab:exp2}
\end{table}

\textbf{Analyse}~: L'augmentation du budget am\'eliore significativement la qualit\'e des trajectoires~: passer de 100 \`a 500 simulations r\'eduit le nombre de pas de moiti\'e (43.4 $\to$ 22.4). Au-del\`a de 500, les rendements sont d\'ecroissants~: doubler le budget (1000) ne gagne que 2.4 pas suppl\'ementaires mais double le temps de calcul. Le budget $N = 500$ offre un bon compromis pour notre taille de grille.

\subsubsection{Exp. 3~: Constante d'exploration}

L'impact de $C$ est mesur\'e avec UCT ($N = 500$)~:

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{$C$} & \textbf{Taux de succ\`es} & \textbf{Pas moyens} \\
\hline
0.5 & 100\% & \textbf{20.1} \\
1.0 & 100\% & 21.4 \\
$\sqrt{2} \approx 1.414$ & 100\% & 23.0 \\
2.0 & 100\% & 22.8 \\
3.0 & 100\% & 23.0 \\
\hline
\end{tabular}
\caption{Impact de la constante d'exploration $C$ sur UCT.}
\label{tab:exp3}
\end{table}

\textbf{Analyse}~: Sur notre sc\'enario, $C = 0.5$ donne les meilleurs r\'esultats. Contrairement au r\'esultat th\'eorique ($C = \sqrt{2}$, optimal pour UCB1), une valeur plus faible favorise l'exploitation dans un espace d'actions restreint (9 directions). Avec seulement 9 actions, l'exploration compl\`ete est rapidement atteinte et un $C$ \'elev\'e gaspille le budget en revisitant des actions d\'ej\`a bien estim\'ees. Ce r\'esultat est coh\'erent avec les observations de \cite{browne2012survey} : la valeur optimale de $C$ d\'epend du probl\`eme.

\subsubsection{Exp. 4~: Environnement dynamique}

Comparaison sur le sc\'enario avec 3 obstacles dynamiques ($N = 500$, 8 r\'ep\'etitions)~:

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Algorithme} & \textbf{Taux de succ\`es} & \textbf{Pas moyens ($\pm\sigma$)} \\
\hline
UCT & 75\% & 31.2 $\pm$ 8.4 \\
RAVE & 87.5\% & 28.6 $\pm$ 6.1 \\
GRAVE & \textbf{87.5\%} & \textbf{27.4 $\pm$ 5.8} \\
\hline
\end{tabular}
\caption{Performance en environnement dynamique. La re-planification \`a chaque pas est essentielle.}
\label{tab:exp4}
\end{table}

\textbf{Analyse}~: Les obstacles dynamiques r\'eduisent le taux de succ\`es (de 100\% \`a 75--87.5\%) et allongent les trajectoires. GRAVE et RAVE maintiennent une performance sup\'erieure gr\^ace \`a l'utilisation plus efficace du budget de simulations via AMAF~: en propageant l'information sur toutes les actions d'un playout, ils apprennent plus vite quelles directions sont dangereuses. La re-planification \`a chaque pas (\textit{receding horizon}) est essentielle car le plan initial devient obsolÃ¨te quand les obstacles bougent.

\subsubsection{Exp. 5~: Bruit capteur}

Robustesse de GRAVE face au bruit capteur ($N = 500$)~:

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Bruit $\sigma$} & \textbf{Taux de succ\`es} & \textbf{Pas moyens} \\
\hline
0\% & 100\% & 22.4 \\
5\% & 100\% & 24.1 \\
10\% & 87.5\% & 28.7 \\
20\% & 75\% & 35.2 \\
30\% & 62.5\% & 42.8 \\
\hline
\end{tabular}
\caption{Impact du bruit capteur sur GRAVE. D\'egradation progressive de la performance.}
\label{tab:exp5}
\end{table}

\textbf{Analyse}~: La performance d\'egrade progressivement avec le bruit. Jusqu'\`a $\sigma = 5\%$, l'impact est n\'egligeable (100\% de succ\`es). \`A $\sigma = 30\%$, le taux de succ\`es chute \`a 62.5\% car le robot peut percevoir des obstacles fant\^omes (faux positifs) ou manquer de vrais obstacles (faux n\'egatifs, plus dangereux). Cela illustre le lien avec les jeux \`a information imparfaite (cours section~19)~: le robot doit prendre des d\'ecisions avec une perception bruit\'ee de l'environnement.

\subsubsection{Exp. 6~: Passage \'etroit}

Sc\'enario le plus difficile~: mur traversant avec un seul passage, obstacle dynamique bloquant ($N = 800$, 8 r\'ep\'etitions)~:

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Algorithme} & \textbf{Taux de succ\`es} & \textbf{Pas moyens} \\
\hline
UCT & 50\% & 58.3 \\
GRAVE & \textbf{62.5\%} & \textbf{52.1} \\
\hline
\end{tabular}
\caption{Passage \'etroit avec obstacle dynamique ($N=800$).}
\label{tab:exp6}
\end{table}

\textbf{Analyse}~: Ce sc\'enario requiert de la \textit{patience}~: le robot doit parfois attendre que l'obstacle dynamique s'\'ecarte pour traverser le passage. GRAVE g\`ere mieux cette situation gr\^ace \`a une meilleure estimation de la valeur de l'action WAIT via les statistiques AMAF. Le budget \'elev\'e ($N = 800$) est n\'ecessaire pour explorer suffisamment de futurs possibles et trouver le bon timing de passage.

% ============================================================================
\section{Discussion}
% ============================================================================

\subsection{Liens avec le cours}

Ce projet met en pratique la plupart des concepts vus en cours~:
\begin{itemize}
  \item \textbf{Flat MC} (section 1.2) comme baseline
  \item \textbf{UCB/UCT} (sections 2--3) comme algorithme principal
  \item \textbf{RAVE/GRAVE} (sections 5--6) pour l'acc\'el\'eration
  \item \textbf{Playout heuristique} inspir\'e de PPA (section 13) et de l'\'echantillonnage de Gibbs (section 10.2)
  \item \textbf{Re-planification} li\'ee aux jeux \`a information imparfaite (section 19)
  \item \textbf{Progressive Widening} (section 16) applicable pour les actions continues
  \item \textbf{Planification multi-mondes} inspir\'ee de PIMC (section 19.2)
  \item \textbf{Planification adaptative} inspir\'ee de Sequential Halving (section 11)
\end{itemize}

\subsection{Limites et perspectives}

\begin{itemize}
  \item \textbf{Espace discret}~: Notre grille est discr\`ete. Pour un robot r\'eel, le Progressive Widening~\citep{couetoux2011continuous} serait n\'ecessaire pour g\'erer les actions continues.
  \item \textbf{Scalabilit\'e}~: Pour de grandes grilles, la parall\'elisation (section 7 du cours) serait b\'en\'efique.
  \item \textbf{Apprentissage}~: L'int\'egration de r\'eseaux de neurones \`a la mani\`ere d'AlphaGo Zero~\citep{silver2017mastering} pourrait am\'eliorer les playouts et l'\'evaluation.
  \item \textbf{Multi-agent}~: L'extension au cas multi-robot n\'ecessiterait des adaptations comme le Pareto MCTS~\citep{chen2019pareto}.
\end{itemize}

\newpage
% ============================================================================
\section{Conclusion}
% ============================================================================

Ce projet d\'emontre l'efficacit\'e des m\'ethodes MCTS pour la planification de trajectoire en environnement dynamique. Sur le sc\'enario statique, tous les algorithmes atteignent 100\% de succ\`es avec des trajectoires quasi-optimales (~22 pas pour une distance Manhattan de 24). Les diff\'erences entre algorithmes deviennent significatives dans les sc\'enarios difficiles~: GRAVE maintient 87.5\% de succ\`es en environnement dynamique (contre 75\% pour UCT) et 62.5\% en passage \'etroit (contre 50\%).

L'\'etude du budget de simulations r\'ev\`ele des rendements d\'ecroissants au-del\`a de $N = 500$, et l'analyse du bruit capteur montre une robustesse acceptable jusqu'\`a $\sigma = 10\%$. La re-planification \`a chaque pas de temps (\textit{receding horizon}) est essentielle pour g\'erer les obstacles dynamiques.

Les r\'esultats confirment que les concepts th\'eoriques des m\'ethodes de Monte Carlo vus en cours (UCT, RAVE, GRAVE, information imparfaite) se transf\`erent efficacement au domaine de la planification robotique.

\newpage
% ============================================================================
\begin{thebibliography}{99}

\bibitem{kocsis2006bandit}
Kocsis, L., Szepesv{\'a}ri, C. (2006).
\textit{Bandit Based Monte-Carlo Planning}.
European Conference on Machine Learning (ECML), pp.~282--293.
DOI:~\href{https://doi.org/10.1007/11871842_29}{10.1007/11871842\_29}

\bibitem{coulom2006efficient}
Coulom, R. (2006).
\textit{Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search}.
5th International Conference on Computers and Games.
DOI:~\href{https://doi.org/10.1007/978-3-540-75538-8_7}{10.1007/978-3-540-75538-8\_7}

\bibitem{browne2012survey}
Browne, C.B., Powley, E., Whitehouse, D., et~al. (2012).
\textit{A Survey of Monte Carlo Tree Search Methods}.
IEEE Trans. on Computational Intelligence and AI in Games, 4(1), pp.~1--43.
DOI:~\href{https://doi.org/10.1109/TCIAIG.2012.2186810}{10.1109/TCIAIG.2012.2186810}

\bibitem{chaslot2008progressive}
Chaslot, G., Winands, M., van den Herik, H., et~al. (2008).
\textit{Progressive Strategies for Monte-Carlo Tree Search}.
New Mathematics and Natural Computation, 4(3), pp.~343--357.
DOI:~\href{https://doi.org/10.1142/S1793005708001094}{10.1142/S1793005708001094}

\bibitem{couetoux2011continuous}
Cou{\"e}toux, A., Hoock, J.B., Sokolovska, N., et~al. (2011).
\textit{Continuous Upper Confidence Trees}.
Learning and Intelligent Optimization (LION), pp.~433--445.
DOI:~\href{https://doi.org/10.1007/978-3-642-25566-3_32}{10.1007/978-3-642-25566-3\_32}

\bibitem{swiechowski2023mcts}
{\'S}wiechowski, M., Godlewski, K., Sawicki, B., Mandziuk, J. (2023).
\textit{Monte Carlo Tree Search: A Review of Recent Modifications and Applications}.
Artificial Intelligence Review, 56, pp.~2497--2562.
DOI:~\href{https://doi.org/10.1007/s10462-022-10228-y}{10.1007/s10462-022-10228-y}

\bibitem{gelly2007combining}
Gelly, S., Silver, D. (2007).
\textit{Combining Online and Offline Knowledge in UCT}.
International Conference on Machine Learning (ICML), pp.~273--280.

\bibitem{cazenave2015generalized}
Cazenave, T. (2015).
\textit{Generalized Rapid Action Value Estimation}.
International Joint Conference on Artificial Intelligence (IJCAI), pp.~754--760.

\bibitem{dam2022monte}
Dam, T.T., Chalvatzaki, G., Peters, J., Pajarinen, J. (2022).
\textit{Monte-Carlo Robot Path Planning}.
IEEE Robotics and Automation Letters, 7(4), pp.~11213--11220.
DOI:~\href{https://doi.org/10.1109/LRA.2022.3199674}{10.1109/LRA.2022.3199674}

\bibitem{li2023self}
Li, W., Liu, Y., Ma, Y., et~al. (2023).
\textit{A Self-Learning Monte Carlo Tree Search Algorithm for Robot Path Planning}.
Frontiers in Neurorobotics, 17, 1039644.
DOI:~\href{https://doi.org/10.3389/fnbot.2023.1039644}{10.3389/fnbot.2023.1039644}

\bibitem{riviere2024sets}
Rivi{\`e}re, B., Lathrop, J., Chung, S.J. (2024).
\textit{MCTS with Spectral Expansion for Planning with Dynamical Systems}.
Science Robotics, 9(97).
DOI:~\href{https://doi.org/10.1126/scirobotics.ado1010}{10.1126/scirobotics.ado1010}

\bibitem{bonanni2025mcts}
Bonanni, L., Meli, D., Castellini, A., Farinelli, A. (2025).
\textit{MCTS with Velocity Obstacles for Safe and Efficient Motion Planning in Dynamic Environments}.
AAMAS 2025.
ArXiv:~\href{https://arxiv.org/abs/2501.09649}{2501.09649}

\bibitem{eiffert2020path}
Eiffert, S., Kong, H., Pirmarzdashti, N., Sukkarieh, S. (2020).
\textit{Path Planning in Dynamic Environments Using Generative RNNs and MCTS}.
IEEE ICRA, pp.~10263--10269.
DOI:~\href{https://doi.org/10.1109/ICRA40945.2020.9196631}{10.1109/ICRA40945.2020.9196631}

\bibitem{janson2015monte}
Janson, L., Schmerling, E., Pavone, M. (2015).
\textit{Monte Carlo Motion Planning for Robot Trajectory Optimization Under Uncertainty}.
ISRR, pp.~343--361.
DOI:~\href{https://doi.org/10.1007/978-3-319-60916-4_20}{10.1007/978-3-319-60916-4\_20}

\bibitem{chen2019pareto}
Chen, W., Liu, L. (2019).
\textit{Pareto Monte Carlo Tree Search for Multi-Objective Informative Planning}.
Robotics: Science and Systems (RSS).

\bibitem{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., et~al. (2017).
\textit{Mastering the Game of Go without Human Knowledge}.
Nature, 550(7676), pp.~354--359.
DOI:~\href{https://doi.org/10.1038/nature24270}{10.1038/nature24270}


\end{thebibliography}
% ============================================================================

\end{document}
